{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729a177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ETL Pipeline Started ===\n",
      "\n",
      "1. EXTRACT: Reading synthetic retail data...\n",
      "  ‚Üí Rows after extraction: 1000\n",
      "\n",
      "2. TRANSFORM: Cleaning and preparing data...\n",
      "  ‚Üí Dropped 0 rows due to missing values\n",
      "  ‚Üí Removed 0 invalid rows (Quantity <= 0 or UnitPrice <= 0)\n",
      "  ‚Üí Filtered to last year (2024-08-12 to 2025-08-12): 376 rows\n",
      "\n",
      "  ‚Üí Building TimeDim...\n",
      "  ‚Üí TimeDim created with 233 unique dates\n",
      "  ‚Üí Building CustomerDim...\n",
      "  ‚Üí CustomerDim created with 298 customers\n",
      "  ‚Üí Building ProductDim...\n",
      "  ‚Üí ProductDim created with 376 products\n",
      "  ‚Üí Building SalesFact...\n",
      "  ‚Üí SalesFact created with 1857 fact records\n",
      "\n",
      "3. LOAD: Writing to SQLite database...\n",
      "‚úÖ ETL completed successfully!\n",
      "üìÅ Database saved as 'retail_dw.db'\n",
      "üìä Fact Table Rows: 1857\n",
      "üåç Customers: 298, Products: 376, Dates: 233\n"
     ]
    }
   ],
   "source": [
    "# etl_retail.py\n",
    "\"\"\"\n",
    "ETL Pipeline for African Retail Data Warehouse\n",
    "Performs:\n",
    "- Extraction: Load synthetic CSV\n",
    "- Transformation: Clean, filter, derive dimensions\n",
    "- Loading: Save to SQLite DB with star schema\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=== ETL Pipeline Started ===\\n\")\n",
    "\n",
    "def run_etl():\n",
    "    # Assume current date is August 12, 2025 (as per exam spec)\n",
    "    current_date = pd.Timestamp('2025-08-12')\n",
    "    one_year_ago = current_date - pd.DateOffset(years=1)\n",
    "\n",
    "    # --- 1. EXTRACT ---\n",
    "    print(\"1. EXTRACT: Reading synthetic retail data...\")\n",
    "    if not os.path.exists('synthetic_retail_africa.csv'):\n",
    "        raise FileNotFoundError(\"synthetic_retail_africa.csv not found. Run generate_retail_data.py first.\")\n",
    "\n",
    "    df = pd.read_csv('synthetic_retail_africa.csv', parse_dates=['InvoiceDate'])\n",
    "    print(f\"  ‚Üí Rows after extraction: {len(df)}\")\n",
    "\n",
    "    # --- 2. TRANSFORM ---\n",
    "    print(\"\\n2. TRANSFORM: Cleaning and preparing data...\")\n",
    "\n",
    "    # Handle missing values\n",
    "    initial_count = len(df)\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"  ‚Üí Dropped {initial_count - len(df)} rows due to missing values\")\n",
    "\n",
    "    # Remove invalid entries\n",
    "    valid_rows = (df['Quantity'] > 0) & (df['UnitPrice'] > 0)\n",
    "    df = df[valid_rows].copy()\n",
    "    print(f\"  ‚Üí Removed {(~valid_rows).sum()} invalid rows (Quantity <= 0 or UnitPrice <= 0)\")\n",
    "\n",
    "    # Add TotalSales\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "    # Filter for last year\n",
    "    recent_mask = df['InvoiceDate'] >= one_year_ago\n",
    "    recent_df = df[recent_mask].copy()\n",
    "    print(f\"  ‚Üí Filtered to last year ({one_year_ago.date()} to {current_date.date()}): {len(recent_df)} rows\")\n",
    "\n",
    "    # Ensure 'Date' column as date type for merging\n",
    "    recent_df['Date'] = recent_df['InvoiceDate'].dt.date\n",
    "\n",
    "    # --- CREATE DIMENSIONS AND FACT ---\n",
    "\n",
    "    # TimeDim: One row per date\n",
    "    print(\"\\n  ‚Üí Building TimeDim...\")\n",
    "    time_dim = recent_df[['InvoiceDate']].drop_duplicates().copy()\n",
    "    time_dim['Date'] = time_dim['InvoiceDate'].dt.date  # Python date\n",
    "    time_dim['TimeKey'] = time_dim['InvoiceDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "    time_dim['Day'] = time_dim['InvoiceDate'].dt.day\n",
    "    time_dim['Month'] = time_dim['InvoiceDate'].dt.month\n",
    "    time_dim['Year'] = time_dim['InvoiceDate'].dt.year\n",
    "    time_dim['Quarter'] = time_dim['InvoiceDate'].dt.to_period('Q').astype(str)\n",
    "    # Keep only necessary columns\n",
    "    time_dim = time_dim[['TimeKey', 'Date', 'Day', 'Month', 'Quarter', 'Year']].drop_duplicates().sort_values('TimeKey').reset_index(drop=True)\n",
    "    print(f\"  ‚Üí TimeDim created with {len(time_dim)} unique dates\")\n",
    "\n",
    "    # CustomerDim: Map CustomerID to details\n",
    "    print(\"  ‚Üí Building CustomerDim...\")\n",
    "    customer_dim = recent_df[['CustomerID', 'Country']].drop_duplicates().reset_index(drop=True)\n",
    "    # Assign names (you can use Faker here if preferred)\n",
    "    customer_dim['Name'] = [f\"Customer_{i}\" for i in range(len(customer_dim))]\n",
    "    # Map African countries to regions\n",
    "    region_map = {\n",
    "        'Nigeria': 'West Africa', 'Ghana': 'West Africa', 'Rwanda': 'East Africa',\n",
    "        'Kenya': 'East Africa', 'Uganda': 'East Africa', 'Tanzania': 'East Africa',\n",
    "        'Ethiopia': 'East Africa', 'South Africa': 'Southern Africa',\n",
    "        'Morocco': 'North Africa', 'Egypt': 'North Africa'\n",
    "    }\n",
    "    customer_dim['Region'] = customer_dim['Country'].map(region_map)\n",
    "    customer_dim.reset_index(inplace=True)\n",
    "    customer_dim.rename(columns={'index': 'CustomerKey'}, inplace=True)\n",
    "    print(f\"  ‚Üí CustomerDim created with {len(customer_dim)} customers\")\n",
    "\n",
    "    # ProductDim: Assign categories\n",
    "    print(\"  ‚Üí Building ProductDim...\")\n",
    "    product_dim = recent_df[['StockCode', 'Description']].drop_duplicates().reset_index(drop=True)\n",
    "    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Toys']\n",
    "    # Deterministic category assignment\n",
    "    np.random.shuffle(categories)  # Shuffle once\n",
    "    category_map = {}\n",
    "    for i, code in enumerate(product_dim['StockCode'].unique()):\n",
    "        category_map[code] = categories[i % len(categories)]\n",
    "    product_dim['Category'] = product_dim['StockCode'].map(category_map)\n",
    "    product_dim.reset_index(inplace=True)\n",
    "    product_dim.rename(columns={'index': 'ProductKey'}, inplace=True)\n",
    "    print(f\"  ‚Üí ProductDim created with {len(product_dim)} products\")\n",
    "\n",
    "    # SalesFact: Join using 'Date' (not InvoiceDate)\n",
    "    print(\"  ‚Üí Building SalesFact...\")\n",
    "    fact = recent_df.merge(\n",
    "        customer_dim[['CustomerID', 'CustomerKey']], on='CustomerID'\n",
    "    ).merge(\n",
    "        product_dim[['StockCode', 'ProductKey']], on='StockCode'\n",
    "    ).merge(\n",
    "        time_dim[['Date', 'TimeKey']], on='Date'  # ‚úÖ Correct: Merge on 'Date'\n",
    "    )\n",
    "\n",
    "    sales_fact = fact[['CustomerKey', 'ProductKey', 'TimeKey', 'Quantity', 'TotalSales']].copy()\n",
    "    print(f\"  ‚Üí SalesFact created with {len(sales_fact)} fact records\")\n",
    "\n",
    "    # --- 3. LOAD ---\n",
    "    print(\"\\n3. LOAD: Writing to SQLite database...\")\n",
    "\n",
    "    db_path = 'retail_dw.db'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Drop existing tables\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript(\"\"\"\n",
    "        DROP TABLE IF EXISTS SalesFact;\n",
    "        DROP TABLE IF EXISTS CustomerDim;\n",
    "        DROP TABLE IF EXISTS ProductDim;\n",
    "        DROP TABLE IF EXISTS TimeDim;\n",
    "    \"\"\")\n",
    "\n",
    "    # Write tables\n",
    "    time_dim.to_sql('TimeDim', conn, if_exists='replace', index=False)\n",
    "    customer_dim.to_sql('CustomerDim', conn, if_exists='replace', index=False)\n",
    "    product_dim.to_sql('ProductDim', conn, if_exists='replace', index=False)\n",
    "    sales_fact.to_sql('SalesFact', conn, if_exists='replace', index=False)\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"‚úÖ ETL completed successfully!\")\n",
    "    print(f\"üìÅ Database saved as '{db_path}'\")\n",
    "    print(f\"üìä Fact Table Rows: {len(sales_fact)}\")\n",
    "    print(f\"üåç Customers: {len(customer_dim)}, Products: {len(product_dim)}, Dates: {len(time_dim)}\")\n",
    "\n",
    "# Run the ETL\n",
    "if __name__ == \"__main__\":\n",
    "    run_etl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
